{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohlDhP9yB4KZ"
   },
   "source": [
    "# DTSC670: Foundations of Machine Learning Models\n",
    "## Assignment 4: Custom Transformer and Transformation Pipeline\n",
    "\n",
    "#### Name:\n",
    "\n",
    "### CodeGrade\n",
    "Note that this assignment will be automatically graded through CodeGrade and you will have unlimited submission attempts.  When submitting to CodeGrade, your notebook should be named `assignment4.ipynb` and there should be no errors in the file or CodeGrade will not be able to grade it.  Before submitting, I suggest that you restart your kernel and attempt to run all cells again to ensure that there will be no errors when CodeGrade runs your script.\n",
    "\n",
    "### Details\n",
    "Your task in this assignment is to create a custom transformation pipeline that takes in raw data and returns fully prepared, clean data that is ready for model training.  However, we will not actually train any models in this assignment.  This pipeline will employ an imputer class, a user-defined transformer class, and a data-normalization class.\n",
    "\n",
    "Please note that the order of features in the final feature matrix must be correct.  See the below figure that illustrates the input and output of the transformation pipeline.  The positions of features $x_1$ and $x_2$ do not change - they remain in the first and second columns, respectively, both before and after the transformation pipeline.  In the transformed dataset, the $x_5$ feature is next, and is followed by the newly computed feature $x_6$.  Finally, the last two columns are the remaining one-hot vectors obtained from encoding the categorical feature $x_3$.\n",
    "\n",
    "<img src=\"DataTransformation.png \" width =\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paEI44LpB4Kg"
   },
   "source": [
    "# Import Data\n",
    "\n",
    "- Make sure that your notebook and data file are located in the same folder since this is what CodeGrade will be expecting.  \n",
    "- Import the data from the file called `CustomTransformerData.csv` and call this DataFrame `custom_transform`.  \n",
    "- Output the `custom_transform` DataFrame to see the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1624045785118,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "iviPNPMhB4Kh",
    "outputId": "67710931-03a9-494d-c85f-e7095ab12143"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>2.354153</td>\n",
       "      <td>COLD</td>\n",
       "      <td>593</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.314048</td>\n",
       "      <td>WARM</td>\n",
       "      <td>340</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>4.021604</td>\n",
       "      <td>COLD</td>\n",
       "      <td>551</td>\n",
       "      <td>4.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLD</td>\n",
       "      <td>2368</td>\n",
       "      <td>6.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>5.847601</td>\n",
       "      <td>WARM</td>\n",
       "      <td>2636</td>\n",
       "      <td>10.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.5</td>\n",
       "      <td>7.229910</td>\n",
       "      <td>WARM</td>\n",
       "      <td>2779</td>\n",
       "      <td>14.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.5</td>\n",
       "      <td>7.997255</td>\n",
       "      <td>HOT</td>\n",
       "      <td>1057</td>\n",
       "      <td>18.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.5</td>\n",
       "      <td>9.203947</td>\n",
       "      <td>COLD</td>\n",
       "      <td>819</td>\n",
       "      <td>24.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.5</td>\n",
       "      <td>10.335348</td>\n",
       "      <td>WARM</td>\n",
       "      <td>3349</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.5</td>\n",
       "      <td>11.112142</td>\n",
       "      <td>HOT</td>\n",
       "      <td>3235</td>\n",
       "      <td>36.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.5</td>\n",
       "      <td>11.759611</td>\n",
       "      <td>WARM</td>\n",
       "      <td>216</td>\n",
       "      <td>44.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.5</td>\n",
       "      <td>12.629096</td>\n",
       "      <td>WARM</td>\n",
       "      <td>2529</td>\n",
       "      <td>52.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.5</td>\n",
       "      <td>14.082589</td>\n",
       "      <td>COLD</td>\n",
       "      <td>1735</td>\n",
       "      <td>60.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.5</td>\n",
       "      <td>14.657678</td>\n",
       "      <td>HOT</td>\n",
       "      <td>1254</td>\n",
       "      <td>70.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOT</td>\n",
       "      <td>1245</td>\n",
       "      <td>80.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.5</td>\n",
       "      <td>17.184114</td>\n",
       "      <td>WARM</td>\n",
       "      <td>310</td>\n",
       "      <td>90.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.5</td>\n",
       "      <td>17.800776</td>\n",
       "      <td>HOT</td>\n",
       "      <td>201</td>\n",
       "      <td>102.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.5</td>\n",
       "      <td>18.578861</td>\n",
       "      <td>HOT</td>\n",
       "      <td>1767</td>\n",
       "      <td>114.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x1         x2    x3    x4          x5\n",
       "0    1.5   2.354153  COLD   593    0.750000\n",
       "1    2.5   3.314048  WARM   340    2.083333\n",
       "2    3.5   4.021604  COLD   551    4.083333\n",
       "3    4.5        NaN  COLD  2368    6.750000\n",
       "4    5.5   5.847601  WARM  2636   10.083333\n",
       "5    6.5   7.229910  WARM  2779   14.083333\n",
       "6    7.5   7.997255   HOT  1057   18.750000\n",
       "7    8.5   9.203947  COLD   819   24.083333\n",
       "8    9.5  10.335348  WARM  3349         NaN\n",
       "9   10.5  11.112142   HOT  3235   36.750000\n",
       "10  11.5  11.759611  WARM   216   44.083333\n",
       "11  12.5  12.629096  WARM  2529   52.083333\n",
       "12  13.5  14.082589  COLD  1735   60.750000\n",
       "13  14.5  14.657678   HOT  1254   70.083333\n",
       "14  15.5        NaN   HOT  1245   80.083333\n",
       "15  16.5  17.184114  WARM   310   90.750000\n",
       "16  17.5  17.800776   HOT   201  102.083333\n",
       "17  18.5  18.578861   HOT  1767  114.083333"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "custom_transform = pd.read_csv('CustomTransformerData.csv')\n",
    "custom_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nopVKUnbB4Kt"
   },
   "source": [
    "# Create Numeric and Categorical DataFrames\n",
    "\n",
    "- Create two new DataFrames: \n",
    "  - Create one DataFrame called `data_num` that holds the numeric features from the `custom_transform` DataFrame.  \n",
    "  - Create another DataFrame called `data_cat` that holds the categorical feature from `custom_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical features\n",
    "numeric_features = ['x1', 'x2', 'x4', 'x5']\n",
    "categorical_feature = 'x3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical and categorical dataframes\n",
    "data_num = custom_transform[numeric_features]\n",
    "data_cat = custom_transform[[categorical_feature]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASUFXY7BB4Km"
   },
   "source": [
    "# Create Custom Transformer\n",
    "\n",
    "Create a custom transformer, just as we did in the lecture video entitled \"Custom Transformers\", that performs two computations: \n",
    "\n",
    "1. Adds an attribute to the end of the numerical data (i.e. new last column) that is equal to $\\frac{x_1^3}{x_5}$ for each observation.  In other words, for each instance, you will cube the $x_1$ column and then divide by the $x_5$ column.\n",
    "\n",
    "2. Drops the entire $x_4$ feature column if the passed function argument `drop_x4` is `True` and doesn't drop the column if `drop_x4` is `False`. (See further instructions below.)\n",
    "\n",
    "You must name your custom transformer class `Assignment4Transformer`. Your class should include an input parameter called `drop_x4` with a default value of `True` that deletes the $x_4$ feature column when its value is `True`, but preserves the $x_4$ feature column when you pass a value of `False`.\n",
    "\n",
    "This transformer will be used in a pipeline. In that pipeline, an imputer will be run *before* this transformer. Keep in mind that the imputer will output an array, so **this transformer must be written to accept an array.**  This is very important and a cause of many errors that students encounter.  In other words, think about using NumPy in your transformer instead of Pandas.\n",
    "\n",
    "Additionally, this transformer will ONLY be given the numerical features of the data. The categorical feature will be handled elsewhere in the full pipeline. This means that your code for this transformer **must reflect the absence of the categorical $x_3$ column** when indexing data structures.  Again this is very important and a cause of the second most number of errors that students encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 848,
     "status": "ok",
     "timestamp": 1624045786162,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "l5UgwTFtB4Kp"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# A lot of this code can be found here: \n",
    "# https://stackoverflow.com/questions/74592893/getting-the-error-assignment4transformer-object-has-no-attribute-y-running-a\n",
    "# I tried to avoid using the code but I can't find any other way to do some of this without using it. I also found help here:\n",
    "# https://www.solveforum.com/forums/threads/solved-custom-transformer-and-transformer-pipeline-in-sklearn.163950/\n",
    "\n",
    "class Assignment4Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_x4=True):\n",
    "        self.drop_x4 = drop_x4\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "# I kept getting indexing errors and my final numpy array always had 5 columns instead of 6. I tried to concatenate it, and ended up with 7 columns. \n",
    "# I tried different ways of coding this, like np.delete and insert, np.column_stack, np.hstack and even a loop. Ultimately, I figured out what \n",
    "# I was doing wrong and I chose to adjust the column indices of the custom transformer and keep the order of operations in the pipeline, which seemed \n",
    "# like what was implied in the instructions about students making the most mistakes here.\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        x6 = X[:, 0]**3 / X[:, 3]  # With x5 now at index 3, performing computation according to assignment specifications\n",
    "        if self.drop_x4:\n",
    "            X = np.delete(X, 2, axis=1) # This assumes x4 is at index 2 after applying the imputer\n",
    "        return np.c_[X, x6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzTuITvAB4Kr"
   },
   "source": [
    "# Create Transformation Pipeline for Numerical Features\n",
    "\n",
    "Create a custom transformation pipeline for only the numeric data called `num_pipeline` that:\n",
    "\n",
    "1. Applies the `SimpleImputer` class to the data, where the strategy is set to `mean`.  The name of this step should be \"imputer\".\n",
    "\n",
    "2. Applies the custom `Assignment4Transformer` class to the data.  Make sure that your custom transformer uses the default argument where you drop the $x_4$ column.  The name of this step should be \"custom_trans\".\n",
    "\n",
    "3. Applies the `StandardScaler` class to the data.  The name of this step should be \"std_scaler\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1624045786486,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "co6dm-JAB4Ks"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create num_pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),  # Fill in the missing values with the mean\n",
    "    ('custom_trans', Assignment4Transformer()),  # Apply the custom transformation\n",
    "    ('std_scaler', StandardScaler()),  # Standardize the features\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLDPD7faB4Kv"
   },
   "source": [
    "# Quick Testing\n",
    "\n",
    "The full pipeline will be implemented with a `ColumnTransformer` class.  However, to be sure that our numeric pipeline is working properly, lets invoke the `fit_transform()` method of the `num_pipeline` object passing it your `data_num` DataFrame.  Save this output data into a variable called `data_num_trans`.\n",
    "\n",
    "### Run Pipeline and Create Transformed Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1624045786493,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "PJknn8GUB4Kw",
    "outputId": "29b16d57-08df-421d-b21a-0c061be1ea20"
   },
   "outputs": [],
   "source": [
    "# Invoke the fit_transform() method on num_pipeline\n",
    "data_num_trans = num_pipeline.fit_transform(data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18MuMZi_B4Kx"
   },
   "source": [
    "### One-Hot Encode Categorical Features\n",
    "\n",
    "Similarly, you will employ a `OneHotEncoder` class in the `ColumnTransformer` below to construct the final full pipeline.  However, let's instantiate an object of the `OneHotEncoder` class to use in the `ColumnTransformer`.\n",
    "\n",
    "1. Call this object `cat_encoder` that has the `drop` parameter set to `first`.  \n",
    "2. Also, include `sparse=False` when instantiating the object to make the auto grading work correctly.  \n",
    "3. Next, call the `fit_transform()` method and pass it your categorical data (`data_cat`).  \n",
    "4. Save this output data into a variable called `data_cat_OHE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyrone\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Instantiate object from the OneHotEncoder class\n",
    "cat_encoder = OneHotEncoder(drop = \"first\", sparse = False)\n",
    "\n",
    "# Pass the categorical data to the fit_transform() method\n",
    "data_cat_OHE = cat_encoder.fit_transform(data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuUbjOPiB4K0"
   },
   "source": [
    "# Put it All Together with a Column Transformer\n",
    "\n",
    "Now, we are finally ready to construct the full transformation pipeline called `full_pipeline` that will transform our raw data into clean, ready-to-train data.  \n",
    "1. Construct this `ColumnTransformer` below.  Note that you will use:\n",
    "  - the `num_pipeline` in the full_pipeline with your numeric data\n",
    "  - the `cat_encoder` object in the full_pipeline with your categorical data\n",
    "2. Call the `fit_transform()` method on the original `custom_transform` data to obtain the final, clean data.  \n",
    "3. Save this output data into a variable called `data_trans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1624045797883,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "7m2SuGOpB4K2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tyrone\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define the column transformer\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_features),\n",
    "    ('cat', cat_encoder, [categorical_feature])\n",
    "])\n",
    "\n",
    "# Apply the full_pipeline on the original data and save object\n",
    "data_trans = full_pipeline.fit_transform(custom_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcJ5AjtB4K3"
   },
   "source": [
    "# Prepare for Grading\n",
    "\n",
    "Prepare your `data_trans` NumPy array for grading by running the code below.  We are using the NumPy [around()](https://numpy.org/doc/stable/reference/generated/numpy.around.html) function to round all the values to 2 decimal places and this will return a NumPy array.\n",
    "\n",
    "Please double check that the final order of the features in this `data_trans` array matches what was given to you at the top of this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1624045786716,
     "user": {
      "displayName": "Ben Pickard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgPExDuYgUieHlogH8E532au3146lVrJ6Ixni9u=s64",
      "userId": "12452112282107072949"
     },
     "user_tz": 240
    },
    "id": "RME1nK-TB4K5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.64, -1.73, -1.2 , -1.59,  0.  ,  0.  ],\n",
       "       [-1.45, -1.53, -1.16, -1.4 ,  0.  ,  1.  ],\n",
       "       [-1.25, -1.38, -1.1 , -1.21,  0.  ,  0.  ],\n",
       "       [-1.06,  0.  , -1.03, -1.02,  0.  ,  0.  ],\n",
       "       [-0.87, -0.99, -0.93, -0.83,  0.  ,  1.  ],\n",
       "       [-0.67, -0.7 , -0.82, -0.64,  0.  ,  1.  ],\n",
       "       [-0.48, -0.53, -0.69, -0.45,  1.  ,  0.  ],\n",
       "       [-0.29, -0.28, -0.54, -0.26,  0.  ,  0.  ],\n",
       "       [-0.1 , -0.04,  0.  , -0.61,  0.  ,  1.  ],\n",
       "       [ 0.1 ,  0.13, -0.18,  0.13,  1.  ,  0.  ],\n",
       "       [ 0.29,  0.27,  0.03,  0.32,  0.  ,  1.  ],\n",
       "       [ 0.48,  0.45,  0.26,  0.51,  0.  ,  1.  ],\n",
       "       [ 0.67,  0.76,  0.5 ,  0.7 ,  0.  ,  0.  ],\n",
       "       [ 0.87,  0.88,  0.76,  0.89,  1.  ,  0.  ],\n",
       "       [ 1.06,  0.  ,  1.05,  1.08,  1.  ,  0.  ],\n",
       "       [ 1.25,  1.42,  1.35,  1.27,  0.  ,  1.  ],\n",
       "       [ 1.45,  1.55,  1.67,  1.46,  1.  ,  0.  ],\n",
       "       [ 1.64,  1.71,  2.01,  1.65,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DO NOT CHANGE THIS CELL BLOCK ###\n",
    "data_trans = np.around(data_trans, decimals=2)\n",
    "\n",
    "data_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_trans: (18, 6)\n",
      "Shape of data_num_trans: (18, 4)\n",
      "Shape of data_cat_OHE: (18, 2)\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the final array matches the the example above\n",
    "print(\"Shape of data_trans:\", data_trans.shape)\n",
    "print(\"Shape of data_num_trans:\", data_num_trans.shape)\n",
    "print(\"Shape of data_cat_OHE:\", data_cat_OHE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_4_complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
