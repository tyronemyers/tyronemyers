{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSC670: Foundations of Machine Learning Models\n",
    "\n",
    "## Assignment 6: Classification System Metrics\n",
    "\n",
    "#### Name:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGrade\n",
    "\n",
    "Note that this assignment will be automatically graded through CodeGrade and you will have unlimited submission attempts.  When submitting to CodeGrade, your notebook should be named `assignment6.ipynb` and there should be no errors in the file or CodeGrade will not be able to grade it.  Before submitting, I suggest that you restart your kernel and attempt to run all cells again to ensure that there will be no errors when CodeGrade runs your script.\n",
    "\n",
    "It is very important that all written functions have the function parameters in the same order as given to you in the respective instructions.  \n",
    "\n",
    "Do not use the built-in Scikit-Learn functions when creating your functions from scratch.  Instead, you may use those functions after to verify your calculations.  Your assignments will be checked and points will be manually taken off if you use Scikit-Learn functions in your created functions.\n",
    "\n",
    "<u style=\"color:red;\">**Important: Do not round any of your outputs or CodeGrade will count them as incorrect**</u>\n",
    "\n",
    "\n",
    "## Assignment Details\n",
    "\n",
    "The purpose of this assignment is to familiarize you with the metrics used to measure prediction performance in classification systems.  Suppose there 20 binary observations whose target values are:\n",
    "\n",
    "$$[1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1]$$\n",
    "\n",
    "Suppose that your machine learning model returns prediction probabilities ([predict_proba()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) in sklearn) of:\n",
    "\n",
    "$$[0.886, 0.375, 0.174, 0.817, 0.574, 0.319, 0.812, 0.314, 0.098, 0.741, 0.847, 0.202, 0.31 , 0.073, 0.179, 0.917, 0.64 , 0.388, 0.116, 0.72]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Model Predictions\n",
    "\n",
    "Begin by writing a function from scratch called `predict()` that accepts as input the following (in this exact order):\n",
    "- a list of prediction probabilities (as a list)\n",
    "- threshold value (as a float)\n",
    "\n",
    "This function should compute the final predictions to be output by the model and return them as a list.  If a prediction probability value is less than or equal to the threshold value, then the prediction is the negative case (i.e. 0).  If a prediction probability value is greater than the threshold value, then the prediction is the positive case (i.e. 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I probably wrote 10 different versions of this function before I felt like I finally had a function that followed the \n",
    "# above logic exactly. Further, I initially wrote it with the parameter names below. However, I changed it so much that \n",
    "# rewriting all the parameter names started becoming very time-consuming. So, I changed  the parameter names to letters \n",
    "# to simplify the function and highlight the logic of the function. Also, I am a trained philosopher and find simple \n",
    "# variables like x, y, and z very clear. Were I writing the code for some other purpose, I would have adhered to best practices and named my parameters to best reflect the purpose of the function.\n",
    "\n",
    "# Given two lists (x and y), create an empty list z\n",
    "def predict(x, y):\n",
    "    z = []\n",
    "    \n",
    "    # For every value in x, if x is less than or equal to y then a 0 is added to the list \n",
    "    for value in x:\n",
    "        if value <= y:\n",
    "            z.append(0)\n",
    "        else:\n",
    "            # For all other values of x, a 1 is added to the list.\n",
    "            z.append(1)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a list of prediction probabilities (as given in the Assignment Details section) called `probs` and a variable called `thresh` that has the value 0.5.  Then invoke the `predict()` function to calculate the model predictions using those variables.  Save this output as `preds` and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction probabilities\n",
    "probs = [0.886,0.375,0.174,0.817,0.574,0.319,0.812,0.314,0.098,0.741,\n",
    "         0.847,0.202,0.31,0.073,0.179,0.917,0.64,0.388,0.116,0.72]\n",
    "\n",
    "# threshold value\n",
    "thresh = 0.5\n",
    "\n",
    "# prediction values\n",
    "preds = predict(probs, thresh)\n",
    "\n",
    "print(\"Model Predictions: \", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Accuracy\n",
    "\n",
    "Write a function from scratch called `acc_score()` that accepts as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions\n",
    "\n",
    "This function should calculate the model accuracy score using the true labels as compared to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I found several ways to do this here: \n",
    "# https://stackoverflow.com/questions/38877301/how-to-calculate-accuracy-based-on-two-lists-python\n",
    "# Obviously, I did not copy anything here, but my function shares similar features.\n",
    "\n",
    "# This function assigns an accuracy score according to the following conditions:\n",
    "\n",
    "# Given two lists (x and y), create a numerical variable z with an initial magnitude of 0 for the accuracy score formula\n",
    "def acc_score(x, y):\n",
    "    z = 0\n",
    "    \n",
    "    # For every x that matches y at the same index, the numerical variable z increases in magnitude by 1.\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == y[i]:\n",
    "            z += 1\n",
    "    \n",
    "    # Then z is divided by the number of objects in x and returned unless \n",
    "    return z / len(x) if len(x) != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the accuracy score using your function `acc_score()`, and pass as input the true labels (listed below as `labels`) and the model predictions you calculated above (`preds`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true labels\n",
    "labels = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "accuracy = acc_score(labels, preds)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Use the Scikit-Learn's [accuracy_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function to check that the value you computed using `acc_score()` is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sklearn_accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "sklearn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Error Rate\n",
    "\n",
    "Write a function from scratch called `error_rate()` that accepts as input (in this exact order):\n",
    "- a list of true labels\n",
    "- a list of model predictions\n",
    "\n",
    "This function should calculate the model error rate and should use your `acc_score()` function that you previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the error rate by subtracting the accuracy score from 1, which is the definition of the error rate. \n",
    "# As above, I changed the parameters to letters to simplify the function. I could have saved the error rate formula and then\n",
    "# returned the saved object, but I chose to calculate the error_rate in the return, instead.\n",
    "\n",
    "def error_rate(x, y):\n",
    "    return 1 - acc_score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the model error rate for the true labels and the model predictions previously given.  Name the error rate that you calculate `error` in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = error_rate(labels, preds)\n",
    "print(\"Model Error Rate: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Precision and Recall\n",
    "\n",
    "Write a function from scratch called `prec_recall_score()` that accepts as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions\n",
    "\n",
    "This function should compute and return _both_ the model precision and recall (in that order).  \n",
    "\n",
    "Do not use the built-in Scikit-Learn functions `precision_score()`,`recall_score()`, `confusion_matrix()`, or Panda's `crosstab()` to do this.  Instead, you may use those functions after to verify your calculations. We want to ensure that you understand what is going on behind-the-scenes of the precision and recall functions by creating similar ones from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates precision and recall scores. Due to the greater complexity of calculating precision and recall,\n",
    "# it seemed prudent to name the numerical values used in the precision and recall formulas accordingly, and to keep the names\n",
    "# of the formulas, as well, in contrast to above. This function uses a similar strategy and logic as the acc_score() function.\n",
    "# It calculates precision and recall according to the following conditions: \n",
    "\n",
    "# Given two lists (x and y), create three numerical variables for the precision and recall equations with an initial\n",
    "# magnitude of 0.\n",
    "def prec_recall_score(x, y):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # For every x and y that are equal to 1 at the same index, the numerical variable tp increases in magnitude by 1; \n",
    "    # For every x equal to 0 and y equal to 1 at the same index, the numerical value fp increases in magnitude by 1; \n",
    "    # For every x equal to 1 an y equal to 0 at the same index, the numerical value fn increases in magnitude by 1.\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 1 and y[i] == 1:\n",
    "            tp += 1\n",
    "        elif x[i] == 0 and y[i] == 1:\n",
    "            fp += 1\n",
    "        elif x[i] == 1 and y[i] == 0:\n",
    "            fn += 1\n",
    "            \n",
    "    # The precision and recall formulas are created and calculated using the numerical values tp, fp, and fn \n",
    "    # (along with a check for division by zero).        \n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `prec_recall_score` function to compute `precision` and `recall` for the true labels and the model predictions you calculated previously.  Save your output as `precision` and `recall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = prec_recall_score(labels, preds)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Use Scikit-Learn's `precision_score()` and `recall_score()` to verify that your calculations above are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "sklearn_precision = precision_score(labels, preds)\n",
    "\n",
    "print(\"Scikit-Learn Precision: \", sklearn_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "sklearn_recall = recall_score(labels, preds)\n",
    "\n",
    "print(\"Scikit-Learn Recall: \", sklearn_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate $F_\\beta$ Scores\n",
    "\n",
    "Write a function from scratch called `f_beta` that computes the $F_\\beta$ measure for any value of $\\beta$.  \n",
    "\n",
    "- This function must invoke the `prec_recall_score` function you wrote above in order to obtain the values for precision and recall.  \n",
    "- The function must take as input (in this exact order):\n",
    "    - a list of true labels\n",
    "    - a list of model predictions you calculated previously\n",
    "    - the value of $\\beta$ you wish to use in the calculation \n",
    "    \n",
    "We defined $F_\\beta$ in class to be:\n",
    "\n",
    "$$ F_\\beta = \\frac{(\\beta^2+1) \\cdot Pr \\cdot Re}{\\beta^2 \\cdot Pr + Re} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the F-beta score according to the following conditions: \n",
    "\n",
    "# Given two lists (x and y) and a beta value\n",
    "def f_beta(x, y, beta):\n",
    "    \n",
    "    # Precision and recall are calculated using the prec_recall_score() function above\n",
    "    precision, recall = prec_recall_score(x, y)\n",
    "    \n",
    "    # A check for zero is performed to prevent division by zero\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # The F-beta score is calculated with the formula: (beta^2 + 1) * (precision * recall) / ((beta^2 * precision) + recall)\n",
    "        # and then returned.\n",
    "        return (beta**2 + 1) * precision * recall / ((beta**2 * precision) + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your `f_beta` function to compute the $F_1$ score for the true labels and the model predictions you calculated previously.  Save your output as `F1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = f_beta(labels, preds, 1)\n",
    "print(\"F1 = \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Verify your above calculation is correct by invoking Scikit-Learn's `f1_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "sklearn_F1 = fbeta_score(labels, preds, beta = 1)\n",
    "\n",
    "sklearn_F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the TPR and FPR for ROC Curve\n",
    "\n",
    "In the subsequent cells, you will be asked to plot an ROC curve.  The ROC curve plots the True Positive Rate (TPR, also called recall) against the False Positive Rate (FPR).  Both of these are scalar values, akin to precision and recall.\n",
    "\n",
    "Write a function from scratch called `TPR_FPR_score` that is nearly identical to `prec_recall_score` that you wrote previously, which computes and returns TPR and FPR (in that order).  The function must take as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions you calculated previously\n",
    "\n",
    "TPR and FPR are defined as follows:\n",
    "\n",
    "$$ TPR = recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the TPR and FPR. As specified in the instructions above, this function is nearly identical to the\n",
    "# prec_recall_score() function. It calculates TPR and FPR according to the following conditions: \n",
    "\n",
    "# Given two lists (x and y), create four numerical variables for the TPR and FPR equations with an initial magnitude of 0.\n",
    "def TPR_FPR_score(x, y): \n",
    "    tp = 0  \n",
    "    fp = 0  \n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    # For every x and y that are equal to 1 at the same index, the numerical variable tp increases in magnitude by 1; \n",
    "    # For every x equal to 0 and y equal to 1 at the same index, the numerical value fp increases in magnitude by 1; \n",
    "    # For every x equal to 1 and y equal to 0 at the same index, the numerical value fn increases in magnitude by 1; \n",
    "    # For every x and y equal to 0 at the same index, the numerical value tn increases in magnitude by 1. \n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 1 and y[i] == 1:\n",
    "            tp += 1     \n",
    "        elif x[i] == 0 and y[i] == 1:\n",
    "            fp += 1\n",
    "        elif x[i] == 1 and y[i] == 0:\n",
    "            fn += 1\n",
    "        elif x[i] == 0 and y[i] == 0:\n",
    "            tn += 1\n",
    "    \n",
    "    # Then, the TPR and FPR functions are created and then calculated using the numerical values tp, fn, fp, and tn \n",
    "    # (with a check for division by zero).\n",
    "    tpr = tp / (tp + fn)   if tp + fn != 0 else 0         \n",
    "    fpr = fp / (fp + tn)  if fp + tn != 0 else 0 \n",
    "\n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Invoke the `TPR_FPR_score` function using your `labels` and `preds` from previous steps.  Your output should be the following:  `(0.875, 0.16666666666666666)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr = TPR_FPR_score(labels, preds)\n",
    "\n",
    "print(\"True Positive Rate = \", tpr)\n",
    "print(\"False Positive Rate = \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and Plot the ROC Curve\n",
    "\n",
    "Write a function from scratch called `roc_curve_computer` that accepts (in this exact order):\n",
    "- a list of true labels\n",
    "- a list of prediction probabilities (notice these are probabilities and not predictions - you will need to obtain the predictions from these probabilities)\n",
    "- a list of threshold values.  \n",
    "\n",
    "The function must compute and return the True Positive Rate (TPR, also called recall) and the False Positive Rate (FPR) for each threshold value in the threshold value list that is passed to the function. \n",
    "\n",
    "**Important:** Be sure to reuse functions and code segments from your work above! You should reuse two of your above created functions so that you do not duplicate your code.  \n",
    "\n",
    "The function you will write behaves identically to Scikit-Learn's [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) function, except that it will take the list of thresholds in as input rather than return them as output.  Your function must calculate one value of TPR and one value of FPR for each of the threshold values in the list.  \n",
    "\n",
    "Your function will output a list of TPR values and a list of FPR values (in that order).  You will then take these TPR and FPR values, and plot them against each other to create the [Receiver Operating Characteristic (ROC) curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).\n",
    "\n",
    "You must not use any built-in library function to perform the calculation of a performance metric.  You may of course use common, built-in Python functions, such as: `range()`, `len()`, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates and returns lists of True Positive and False Positive Rates (tprs and fprs) for plotting an ROC\n",
    "# curve at different thresholds. I would have explained this like the above, but I built the function using the \"pseudo code\"\n",
    "# from the DTSC 670 FAQ. So, I am following the logic of those instructions explicitly, and I went ahead and inserted the \n",
    "# instructions as comments into the function.\n",
    "\n",
    "def roc_curve_computer(x, y, z):\n",
    "    # Create two empty lists and call them 'TPR' and 'FPR'\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "\n",
    "    # Loop through each threshold in your thresholds list.\n",
    "    for t in z:\n",
    "        # Pass the respective threshold along with your probabilities to your predict function and save the output as 'preds'\n",
    "        preds = predict(y, t)\n",
    "\n",
    "        # Pass the true labels and the preds that you created in step 3 to your TPR_FPR_score function saving the output as \n",
    "        # 'tpr' and 'fpr'\n",
    "        tpr, fpr = TPR_FPR_score(x, preds)\n",
    "\n",
    "        # Append those values to the respective lists that you created in step 1\n",
    "        TPR.append(tpr)\n",
    "        FPR.append(fpr)\n",
    "\n",
    "    # Keep looping through the threshold list, going back to step 2 and repeating the process until you have gone through each \n",
    "    # threshold\n",
    "\n",
    "    # Return your 'TPR' and 'FPR' lists\n",
    "    return TPR, FPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** As an example, calling the `roc_curve_computer` function with the input `true_labels = [1, 0, 1, 0, 0]`, `pred_probs = [0.875, 0.325, 0.6, 0.09, 0.4]`, and `thresholds = [0.00, 0.25, 0.50, 0.75, 1.00]` yields the output:\n",
    "\n",
    "`TPR =  [1.0, 1.0, 1.0, 0.5, 0.0]` and `FPR =  [1.0, 0.6666, 0.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [1, 0, 1, 0, 0]\n",
    "pred_probs = [0.875, 0.325, 0.6, 0.09, 0.4]\n",
    "thresholds = [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "tpr_values, fpr_values = roc_curve_computer(true_labels, pred_probs, thresholds)\n",
    "\n",
    "print(\"TPR values: \", tpr_values)\n",
    "print(\"FPR values: \", fpr_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use your `roc_curve_computer` function along with the threshold values `thresholds = [x/100 for x in range(101)]` to compute the TPR and FPR lists for the provided data and save your output as `TPR` and `FPR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [x/100 for x in range(101)]\n",
    "TPR, FPR = roc_curve_computer(labels, probs, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following plotting function to plot the ROC curve.  Pass the TPR and FPR values that you calculated above into the plotting function to view the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal line\n",
    "    plt.title('Receiver Operating Characteristic', fontsize=12)\n",
    "    plt.axis([-0.015, 1.0, 0, 1.02])\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_roc_curve(TPR, FPR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Next, compare your plot to the plot generated by Scikit-Learn's `roc_curve` function.  Use Scikit-Learn's `roc_curve` function to calculate the false positive rates, the true positive rates, and the thresholds.  Save the output using sklearn's function as `fpr`, `tpr`, and `thresholds`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, pred_probs)\n",
    "\n",
    "print(\"TPR values: \", tpr)\n",
    "print(\"FPR values: \", fpr)\n",
    "print(\"Thresholds: \", thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the false positive rates and the true positive rates obtained above via the Scikit-Learn function as input to the `plot_roc_curve` function in order to compare ROC curves. These two plots should look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plot_roc_curve(tpr, fpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
